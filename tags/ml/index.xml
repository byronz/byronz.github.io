<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on 雜</title>
    <link>https://byronz.github.io/tags/ml/</link>
    <description>Recent content in Ml on 雜</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 | 朱曉清 | &lt;small&gt;powered by&lt;/small&gt; &lt;a href=&#34;gohugo.io&#34; target=&#34;_blank&#34;&gt;Hugo&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 18 Nov 2017 23:08:20 -0500</lastBuildDate>
    
	<atom:link href="https://byronz.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Ski Resort Clusterring near Montreal</title>
      <link>https://byronz.github.io/posts/ski/</link>
      <pubDate>Sat, 18 Nov 2017 23:08:20 -0500</pubDate>
      
      <guid>https://byronz.github.io/posts/ski/</guid>
      <description>Rational My 3rd ski season in montérégie is coming, and this post uses a very basic machine-learning-unsupervised-clusterring algorithm to show an interesting analysis about how to choose your next ski journey.
Algorithm Quote from wiki page
 k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.</description>
    </item>
    
    <item>
      <title>ML Week4 - Neural Network</title>
      <link>https://byronz.github.io/posts/ml4/</link>
      <pubDate>Tue, 24 Oct 2017 22:36:04 -0400</pubDate>
      
      <guid>https://byronz.github.io/posts/ml4/</guid>
      <description>Brain  auditory cortex somatosensory cortex  NN Model  Neuro.IO
Dentrite =&amp;gt; Axon
input layer &amp;gt; hidden layer (intermediate layer) &amp;gt; output layer
 all hidden layer nodes are called &amp;ldquo;activation units&amp;rdquo;
$$a_i^{(j)} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$}$$ $$\Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}$$
Forward Propagation using trained parameters $\theta_{n}$ we can predict the output by calculate the output layer value</description>
    </item>
    
    <item>
      <title>ML Week2</title>
      <link>https://byronz.github.io/posts/ml_w2/</link>
      <pubDate>Sun, 08 Oct 2017 20:08:02 -0400</pubDate>
      
      <guid>https://byronz.github.io/posts/ml_w2/</guid>
      <description>Multivariate Linear Regression Hypothesis $$ h_\theta(x) = \begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} &amp;hellip; \hspace{2em} \theta_n \end{bmatrix} \begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix} = \theta^T x $$
Gradient Descent Practices
 Scaling or normalize the range (e.g. mean normalization) learning rate $\alpha$  Ploynomial Regression typical equations:
 $\theta_0x_0 + \theta_1x_1 + \theta_2x_2^2$ $\theta_0x_0 + \theta_1x_1 + \theta_2x_2^2 + \theta_3x_3^3$ $\theta_0x_0 + \theta_1x_1 + \theta_2 \sqrt{x_2}$  Normal Equation if n &amp;gt; 1000, hard to solve the inverse of $(X^TX)^{-1}$</description>
    </item>
    
    <item>
      <title>ML Week1</title>
      <link>https://byronz.github.io/posts/ml/</link>
      <pubDate>Fri, 06 Oct 2017 21:58:37 -0400</pubDate>
      
      <guid>https://byronz.github.io/posts/ml/</guid>
      <description>Andrew Ng&amp;rsquo;s ML Course Note Diagram Supervised Learning In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into &amp;ldquo;regression&amp;rdquo; and &amp;ldquo;classification&amp;rdquo; problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying</description>
    </item>
    
  </channel>
</rss>